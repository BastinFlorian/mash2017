{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning In-Depth: Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we saw the main API shared by objects in scikit-learn and a couple of estimators, notably logistic regression and K-nearest neighbors.\n",
    "\n",
    "In this session we will take a look at model selection, one of the most important aspects of applied machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some imports\n",
    "\n",
    "# plotting, set up plotly offline\n",
    "from plotly.offline import init_notebook_mode, iplot, download_plotlyjs\n",
    "init_notebook_mode()\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting, Underfitting and Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The issues associated with validation and \n",
    "cross-validation are some of the most important\n",
    "aspects of the practice of machine learning.  Selecting the optimal model\n",
    "for your data is vital, and is a piece of the problem that is not often\n",
    "appreciated by machine learning practitioners.\n",
    "\n",
    "Of core importance is the following question:\n",
    "\n",
    "**If our estimator is underperforming, how should we move forward?**\n",
    "\n",
    "- Use simpler or more complicated model?\n",
    "- Add more training samples?\n",
    "\n",
    "The answer is often counter-intuitive.  In particular, **Sometimes using a\n",
    "more complicated model will give _worse_ results.**  Also, **Sometimes adding\n",
    "training data will not improve your results.**  The ability to determine\n",
    "what steps will improve your model is what separates the successful machine\n",
    "learning practitioners from the unsuccessful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of the Bias-Variance Tradeoff\n",
    "\n",
    "For this section, we'll work with a simple 1D regression problem.  This will help us to\n",
    "easily visualize the data and the model, and the results generalize easily to  higher-dimensional\n",
    "datasets.  We'll explore a simple **linear regression** problem.\n",
    "This can be accomplished within scikit-learn with the `sklearn.linear_model` module.\n",
    "\n",
    "We'll create a simple nonlinear function that we'd like to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_func(x, err=0.5):\n",
    "    y = 10 - 1. / (x + 0.1)\n",
    "    if err > 0:\n",
    "        y = np.random.normal(y, err)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a realization of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_data(N=40, error=1.0, random_seed=1):\n",
    "    # randomly sample the data\n",
    "    np.random.seed(1)\n",
    "    X = np.random.random(N)[:, np.newaxis]\n",
    "    y = test_func(X.ravel(), error)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_data(40, error=1)\n",
    "iplot([go.Scatter(x=X.ravel(), y=y, mode='markers')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now say we want to perform a regression on this data.  Let's use the built-in linear regression function to compute a fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)))\n",
    "iplot([go.Scatter(x=X.ravel(), y=y, mode='markers'),\n",
    "        go.Scatter(x=X_test.ravel(), y=y_test, name='estimated model')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have fit a straight line to the data, but clearly this model is not a good choice.  We say that this model is **biased**, or that it **under-fits** the data.\n",
    "\n",
    "Let's try to improve this by creating a more complicated model.  We can do this by adding degrees of freedom, and computing a polynomial regression over the inputs. Scikit-learn makes this easy with the ``PolynomialFeatures`` preprocessor, which can be pipelined with a linear regression.\n",
    "\n",
    "Let's make a convenience routine to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use this to fit a quadratic curve to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(2)\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)))\n",
    "\n",
    "iplot([go.Scatter(x=X.ravel(), y=y, mode='markers'),\n",
    "        go.Scatter(x=X_test.ravel(), y=y_test, name='estimated model')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduces the mean squared error, and makes a much better fit.  What happens if we use an even higher-degree polynomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(10)\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "print(\"mean squared error: {0:.3g}\".format(mean_squared_error(model.predict(X), y)))\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "#     yaxis=dict(\n",
    "#         range=[0, 20]\n",
    "#     )\n",
    ")\n",
    "data = [go.Scatter(x=X.ravel(), y=y, mode='markers', name='original data'),\n",
    "        go.Scatter(x=X_test.ravel(), y=y_test, name='estimated model')]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we increase the degree to this extent, it's clear that the resulting fit is no longer reflecting the true underlying distribution, but is more sensitive to the noise in the training data. For this reason, we call it a **high-variance model**, and we say that it **over-fits** the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Over-fitting with Validation Curves\n",
    "\n",
    "Clearly, computing the error on the training data is not enough (we saw this previously). As above, we can use **cross-validation** to get a better handle on how the model fit is working.\n",
    "\n",
    "Let's do this here, again using the ``validation_curve`` utility. To make things more clear, we'll use a slightly larger dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = make_data(120, error=1.0)\n",
    "iplot([go.Scatter(x=X.ravel(), y=y, mode='markers')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "degree = np.arange(0, 15)\n",
    "val_train, val_test = validation_curve(PolynomialRegression(), X, y,\n",
    "                                       'polynomialfeatures__degree', degree, \n",
    "                                       cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_train.mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the validation curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(val_train, val_test):\n",
    "    data = [\n",
    "        go.Scatter(\n",
    "            x=degree,\n",
    "            y=val_train.mean(1),\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=val_train.std(1),\n",
    "                visible=True\n",
    "            ),\n",
    "            name='train error'\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=degree,\n",
    "            y=val_test.mean(1),\n",
    "            error_y=dict(\n",
    "                type='data',\n",
    "                array=val_train.std(1),\n",
    "                visible=True\n",
    "            ),\n",
    "            name='validation error'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        xaxis=dict(\n",
    "            title='Polynomial degree',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Score',\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    iplot(fig)\n",
    "\n",
    "plot_learning_curves(val_train, val_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the trend here, which is common for this type of plot.\n",
    "\n",
    "1. For a small model complexity, the training error and validation error are very similar. This indicates that the model is **under-fitting** the data: it doesn't have enough complexity to represent the data. Another way of putting it is that this is a **high-bias** model.\n",
    "\n",
    "2. As the model complexity grows, the training and validation scores diverge. This indicates that the model is **over-fitting** the data: it has so much flexibility, that it fits the noise rather than the underlying trend. Another way of putting it is that this is a **high-variance** model.\n",
    "\n",
    "3. Note that the training score (nearly) always improves with model complexity. This is because a more complicated model can fit the noise better, so the model improves. The validation data generally has a sweet spot, which here is around 5 terms.\n",
    "\n",
    "# <span style='color: red'>__Tip__</span>: in the final report you __absolutely__ must include these plots.\n",
    "\n",
    "Here's our best-fit model according to the cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = PolynomialRegression(4).fit(X, y)\n",
    "\n",
    "iplot([go.Scatter(x=X.ravel(), y=y, mode='markers'),\n",
    "        go.Scatter(x=X_test.ravel(), y=model.predict(X_test), name='estimated model')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (10')\n",
    "\n",
    "On the digits dataset, plot the validation curve for the ```C``` parameter in ```LogisticRegression```, with 10 values between $10^-6$ and $10^6$ in log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup code for exercise\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically choose the best hyperparameter\n",
    "\n",
    "We can use the above curves to choose the best hyperparameters for our model. We can simply iterate through the validation curve and select the best hyperparameter. However, there is a better way in scikit-learn: through the ```GridSearchCV``` object.\n",
    "\n",
    "This is an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# get some data\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# define the different hyperparameters\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "print(Cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call GridSearchCV with a LogisticRegression estimator, but choosing\n",
    "# C out of all Cs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = GridSearchCV(estimator=LogisticRegression(), param_grid=dict(C=Cs))\n",
    "clf.fit(X, y)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So under the hood, ```GridSearchCV``` has:\n",
    "\n",
    "  * Tried out all values of ```C``` in Cs.\n",
    "  * Selected by cross-validation the one with best score.\n",
    "\n",
    "Once fitted, the ```GridSearchCV``` behaves like any other estimator. Like any other estimator it implements a .predict and .score method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```GridSearchCV``` is what we call a **meta-estimator**: it behaves like an estimator but it is in fact made up from one or several estimators. This is a powerful concept!\n",
    "\n",
    "How to retrieve the optimal value of C that was selected by ```GridSearchCV```? ```GridSearchCV``` has a member ```clf.best_estimator_``` that returns which was the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "\n",
    "**Multi-layer Perceptron (MLP)** is a supervised learning algorithm that learns\n",
    "a function $f(\\cdot): R^m \\rightarrow R^o$ by training on a dataset,\n",
    "where $m$ is the number of dimensions for input and $o$ is the\n",
    "number of dimensions for output. Given a set of features $X = \\{x_1, x_2, ..., x_m\\}$\n",
    "and a target $y$, it can learn a non-linear function approximator for either\n",
    "classification or regression. It is different from logistic regression, in that\n",
    "between the input and the output layer, there can be one or more non-linear\n",
    "layers, called hidden layers. This represents one hidden layer MLP with scalar\n",
    "output.\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/multilayerperceptron_network.png\" width=400/>\n",
    "\n",
    "\n",
    "On the digits dataset, build a neural network model, [MLPCLassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) and:\n",
    "  * choose the best possible parameters of ```alpha``` using ```GridSearchCV```.\n",
    "  * choose the best possible parameter for ```hidden_layer_sizes```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines or combining different classifiers\n",
    "\n",
    "At some point, you might want to combine some preprocessing (like feature selection) with a classifier. It is possible to do this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get some data\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "\n",
    "from sklearn import decomposition\n",
    "# select only 10 features\n",
    "pca = decomposition.PCA(n_components=10)\n",
    "X_trans = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# now train the classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_trans, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this soon becomes tedious when dealing with cross-validation. A better solution is to create a Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "clf = make_pipeline(\n",
    "    decomposition.PCA(n_components=10), LogisticRegression())\n",
    "clf.fit(X, y)\n",
    "print(clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a Pipeline with PCA and LogisticRegression, plot performance as the number of dimensions (```n_components```) by cross-validation and get the optimum using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've gone over several useful tools for model validation\n",
    "\n",
    "- The **Training Score** shows how well a model fits the data it was trained on. This is not a good indication of model effectiveness\n",
    "- The **Validation Score** shows how well a model fits hold-out data. The most effective method is some form of cross-validation, where multiple hold-out sets are used.\n",
    "- **Validation Curves** are a plot of validation score and training score as a function of **model complexity**:\n",
    "  + when the two curves are close, it indicates *underfitting*\n",
    "  + when the two curves are separated, it indicates *overfitting*\n",
    "  + the \"sweet spot\" is in the middle\n",
    "- **GridSearchCV** is an object that encapsulates the logic of performing model selection.\n",
    "  + By specifying the different values of the parameter, he will try them out and keep the best.\n",
    "  + The resulting object is a \n",
    "  \n",
    "These tools are powerful means of evaluating your model on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by Fabian Pedregosa based on work by [Jake Vanderplas](http://www.vanderplas.com).</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
